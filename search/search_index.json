{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"dike Python asyncio tools for web service resilience Documentation: https://chr1st1ank.github.io/dike/ Features Retry decorator for asynchronous functions A very common task especially for network calls is an automatic retry with proper exception logging. There are good implementations like the retry package for classic functions. But dike provides a similar implementation for coroutine functions. This is available with the @retry decorator. Simplified example: import asyncio import datetime import logging import sys import dike @dike.retry(attempts=2, delay=datetime.timedelta(milliseconds=10), exception_types=RuntimeError) async def web_request(): raise RuntimeError(\"Request failed!\") async def main(): response = await web_request() print(response) logging.basicConfig(stream=sys.stdout) asyncio.run(main()) The output shows first a warning log message including the exception info (that's configurable). This is especially useful if you use structured logging. # Log for first attempt: WARNING:dike:Caught exception RuntimeError('Request failed!'). Retrying in 0.01s ... Traceback (most recent call last): ... RuntimeError: Request failed! Then, the for the final failure the exception is propagated to the function caller: Traceback (most recent call last): ... RuntimeError: Request failed! Process finished with exit code 1 Concurrency limiting for asynchronous functions The @limit_jobs decorator allows to limit the number of concurrent excecutions of a coroutine function. This can be useful for limiting queueing times or for limiting the load put onto backend services. Example with an external web request using the httpx library: import asyncio import dike import httpx @dike.limit_jobs(limit=2) async def web_request(): \"\"\"Sends a slow web request\"\"\" async with httpx.AsyncClient() as client: response = await client.get(\"https://httpbin.org/status/200?sleep=100\") return response async def main(): # Send three requests at the same time call1 = web_request() call2 = web_request() call3 = web_request() responses = await asyncio.gather(call1, call2, call3, return_exceptions=True) # Print the responses for r in responses: print(r) asyncio.run(main()) The output shows that the first two requests succeed. The third one hits the concurrency limit and a TooManyCalls exception is returned: <Response [200 OK]> <Response [200 OK]> Too many calls to function web_request! limit=2 exceeded Mini-batching for asynchronous function calls The @batch decorator groups function calls into batches and only calls the wrapped function with the aggregated input. This is useful if the function scales well with the size of the input arguments but you're getting the input data in smaller bits, e.g. as individual HTTP requests. The arguments can be batched together as a Python list or optionally also as numpy array. Example: import asyncio import dike @dike.batch(target_batch_size=3, max_waiting_time=10) async def add_args(arg1, arg2): \"\"\"Elementwise sum of the values in arg1 and arg2\"\"\" print(f\"arg1: {arg1}\") print(f\"arg2: {arg2}\") return [a1 + a2 for a1, a2 in zip(arg1, arg2)] async def main(): result = await asyncio.gather( add_args([0], [1]), add_args([1], [1]), add_args([2, 3], [1, 1]), ) print(f\"Result: {result}\") asyncio.run(main()) Output: arg1: [0, 1, 2, 3] arg2: [1, 1, 1, 1] Result: [[1], [2], [3, 4]] Installation Simply install from pypi. The library is pure Python without any dependencies other than the standard library. pip install dike","title":"Home"},{"location":"#dike","text":"Python asyncio tools for web service resilience Documentation: https://chr1st1ank.github.io/dike/","title":"dike"},{"location":"#features","text":"","title":"Features"},{"location":"#retry-decorator-for-asynchronous-functions","text":"A very common task especially for network calls is an automatic retry with proper exception logging. There are good implementations like the retry package for classic functions. But dike provides a similar implementation for coroutine functions. This is available with the @retry decorator. Simplified example: import asyncio import datetime import logging import sys import dike @dike.retry(attempts=2, delay=datetime.timedelta(milliseconds=10), exception_types=RuntimeError) async def web_request(): raise RuntimeError(\"Request failed!\") async def main(): response = await web_request() print(response) logging.basicConfig(stream=sys.stdout) asyncio.run(main()) The output shows first a warning log message including the exception info (that's configurable). This is especially useful if you use structured logging. # Log for first attempt: WARNING:dike:Caught exception RuntimeError('Request failed!'). Retrying in 0.01s ... Traceback (most recent call last): ... RuntimeError: Request failed! Then, the for the final failure the exception is propagated to the function caller: Traceback (most recent call last): ... RuntimeError: Request failed! Process finished with exit code 1","title":"Retry decorator for asynchronous functions"},{"location":"#concurrency-limiting-for-asynchronous-functions","text":"The @limit_jobs decorator allows to limit the number of concurrent excecutions of a coroutine function. This can be useful for limiting queueing times or for limiting the load put onto backend services. Example with an external web request using the httpx library: import asyncio import dike import httpx @dike.limit_jobs(limit=2) async def web_request(): \"\"\"Sends a slow web request\"\"\" async with httpx.AsyncClient() as client: response = await client.get(\"https://httpbin.org/status/200?sleep=100\") return response async def main(): # Send three requests at the same time call1 = web_request() call2 = web_request() call3 = web_request() responses = await asyncio.gather(call1, call2, call3, return_exceptions=True) # Print the responses for r in responses: print(r) asyncio.run(main()) The output shows that the first two requests succeed. The third one hits the concurrency limit and a TooManyCalls exception is returned: <Response [200 OK]> <Response [200 OK]> Too many calls to function web_request! limit=2 exceeded","title":"Concurrency limiting for asynchronous functions"},{"location":"#mini-batching-for-asynchronous-function-calls","text":"The @batch decorator groups function calls into batches and only calls the wrapped function with the aggregated input. This is useful if the function scales well with the size of the input arguments but you're getting the input data in smaller bits, e.g. as individual HTTP requests. The arguments can be batched together as a Python list or optionally also as numpy array. Example: import asyncio import dike @dike.batch(target_batch_size=3, max_waiting_time=10) async def add_args(arg1, arg2): \"\"\"Elementwise sum of the values in arg1 and arg2\"\"\" print(f\"arg1: {arg1}\") print(f\"arg2: {arg2}\") return [a1 + a2 for a1, a2 in zip(arg1, arg2)] async def main(): result = await asyncio.gather( add_args([0], [1]), add_args([1], [1]), add_args([2, 3], [1, 1]), ) print(f\"Result: {result}\") asyncio.run(main()) Output: arg1: [0, 1, 2, 3] arg2: [1, 1, 1, 1] Result: [[1], [2], [3, 4]]","title":"Mini-batching for asynchronous function calls"},{"location":"#installation","text":"Simply install from pypi. The library is pure Python without any dependencies other than the standard library. pip install dike","title":"Installation"},{"location":"api/","text":"API documentation Decorators function batch ( target_batch_size , max_waiting_time , max_processing_time=10.0 , argument_type='list' ) @batch is a decorator to cumulate function calls and process them in batches. Not thread-safe. The function to wrap must have arguments of type list or numpy.array which can be aggregated. It must return just a single value of the same type. The type has to be specified with the argument_type parameter of the decorator. Parameters target_batch_size (int) \u2014 As soon as the collected function arguments reach target_batch_size, the wrapped function is called and the results are returned. Note that the function may also be called with longer arguments than target_batch_size. max_waiting_time (float) \u2014 Maximum waiting time in seconds before calling the underlying function although the target_batch_size hasn't been reached. max_processing_time (float, optional) \u2014 Maximum time in seconds for the processing itself (without waiting) before an asyncio.TimeoutError is raised. Note: It is strongly advised to set a reasonably strict timeout here in order not to create starving tasks which never finish in case something is wrong with the backend call. argument_type (str, optional) \u2014 The type of function argument used for batching. One of \"list\" or \"numpy\". Per default \"list\" is used, i.e. it is assumed that the input arguments to the wrapped functions are lists which can be concatenated. If set to \"numpy\" the arguments are assumed to be numpy arrays which can be concatenated by numpy.concatenate() along axis 0. Raises ValueError \u2014 If the arguments target_batch_size or max_waiting time are not >= 0 or if the argument_type is invalid. ValueError \u2014 When calling the function with incorrect or inconsistent arguments. asyncio.TimeoutError \u2014 Is raised when calling the wrapped function takes longer than max_processing_time Returns (callable(callable(...: ): callable(...: ))) A coroutine function which executed the wrapped function with batches of input arguments. The wrapped function is called with concatenated arguments of multiple calls. Notes The decorator is not thread-safe, but only allows for concurrent access by async functions. The wrapped function may use multithreading, but only one thread at a time may call the function in order to avoid race conditions. The return value of the wrapped function must be a single iterable. All calls to the underlying function need to have the same number of positional arguments and the same keyword arguments. It also isn't possible to mix the two ways to pass an argument. The same argument always has to be passed either as keyword argument or as positional argument. Example >>> import asyncio >>> import dike ... ... >>> @dike.batch(target_batch_size=3, max_waiting_time=10) ... async def f(arg1, arg2): ... print(f\"arg1: {arg1}\") ... print(f\"arg2: {arg2}\") ... return [10, 11, 12] ... ... >>> async def main(): ... result = await asyncio.gather( ... f([0], [\"a\"]), ... f([1], [\"b\"]), ... f([2], [\"c\"]), ... ) ... ... print(f\"Result: {result}\") ... ... >>> asyncio.run(main()) arg1: [0, 1, 2] arg2: ['a', 'b', 'c'] Result: [[10], [11], [12]] function limit_jobs ( limit ) Decorator to limit the number of concurrent calls to a coroutine function. Not thread-safe. Parameters limit (int) \u2014 The maximum number of ongoing calls allowed at any time Returns (callable(...: )) The given coroutine function with added concurrency protection Raises TooManyCalls \u2014 The decorated function raises a dike.ToomanyCalls exception if it is called while already running limit times concurrently. ValueError \u2014 If the decorator is applied to something else than an async def function Note The decorator is not thread-safe, but only allows for concurrent access by async functions. The wrapped function may use multithreading, but only one thread at a time may call the function in order to avoid race conditions. Examples >>> import dike >>> import asyncio >>> import httpx >>> import dike ... ... >>> @dike.limit_jobs(limit=2) ... async def web_request(): ... async with httpx.AsyncClient() as client: ... response = await client.get(\"https://httpbin.org/status/200?sleep=100\") ... return response ... ... >>> async def main(): ... responses = await asyncio.gather( ... web_request(), web_request(), web_request(), return_exceptions=True ... ) ... for r in responses: ... if isinstance(r, dike._limit_jobs.TooManyCalls): ... print(\"too many calls\") ... else: ... print(r) ... ... >>> asyncio.run(main()) <Response [200 OK]> <Response [200 OK]> too many calls function retry ( attempts=None , exception_types=<class 'Exception'> , delay=None , backoff=1 , log_exception_info=True ) Decorator to limit the number of concurrent calls to a coroutine function. Not thread-safe. Parameters attempts (int, optional) \u2014 The maximum number of tries before re-raising the last exception. Per default there is no limit. exception_types (Union(type of baseexception, (type of baseexception)), optional) \u2014 The exception types which allow a retry. In case of other exceptions there is no retry. delay (timedelta, optional) \u2014 Delay between attempts. Per default there is no delay. backoff (int, optional) \u2014 Multiplier applied to the delay between attempts. Per default this is 1 , so that the delay is constant and does not grow. E.g. use 2 to double the delay with each attempt. log_exception_info (bool, optional) \u2014 Wether to include the exception stacktrace when logging any failed attempts. Default: True Returns (callable(callable(...: ): callable(...: ))) The given coroutine function with added exception handling and retry logic. Raises ValueError \u2014 For invalid configuration arguments. Examples >>> import asyncio >>> import httpx >>> import datetime >>> import dike ... >>> @dike.retry(attempts=2, delay=datetime.timedelta(milliseconds=10)) ... async def web_request(): ... async with httpx.AsyncClient() as client: ... response = await client.get(\"https://httpbin.org/status/400\") ... if response.status_code != httpx.codes.OK: ... raise RuntimeError(\"Request failed!\") ... return response ... ... >>> async def main(): ... response = await web_request() ... print(response) ... ... >>> asyncio.run(main()) ... # Log messages from two attempts: ... # WARNING:dike:Caught exception RuntimeError('Request failed!'). Retrying in 0.01s ... ... # Then: Traceback (most recent call last): ... RuntimeError: Request failed! Exceptions class TooManyCalls ( ) Bases Exception BaseException Error raised by @limit_jobs when a call exceeds the preset limit.","title":"API documentation"},{"location":"api/#api-documentation","text":"","title":"API documentation"},{"location":"api/#decorators","text":"function batch ( target_batch_size , max_waiting_time , max_processing_time=10.0 , argument_type='list' ) @batch is a decorator to cumulate function calls and process them in batches. Not thread-safe. The function to wrap must have arguments of type list or numpy.array which can be aggregated. It must return just a single value of the same type. The type has to be specified with the argument_type parameter of the decorator. Parameters target_batch_size (int) \u2014 As soon as the collected function arguments reach target_batch_size, the wrapped function is called and the results are returned. Note that the function may also be called with longer arguments than target_batch_size. max_waiting_time (float) \u2014 Maximum waiting time in seconds before calling the underlying function although the target_batch_size hasn't been reached. max_processing_time (float, optional) \u2014 Maximum time in seconds for the processing itself (without waiting) before an asyncio.TimeoutError is raised. Note: It is strongly advised to set a reasonably strict timeout here in order not to create starving tasks which never finish in case something is wrong with the backend call. argument_type (str, optional) \u2014 The type of function argument used for batching. One of \"list\" or \"numpy\". Per default \"list\" is used, i.e. it is assumed that the input arguments to the wrapped functions are lists which can be concatenated. If set to \"numpy\" the arguments are assumed to be numpy arrays which can be concatenated by numpy.concatenate() along axis 0. Raises ValueError \u2014 If the arguments target_batch_size or max_waiting time are not >= 0 or if the argument_type is invalid. ValueError \u2014 When calling the function with incorrect or inconsistent arguments. asyncio.TimeoutError \u2014 Is raised when calling the wrapped function takes longer than max_processing_time Returns (callable(callable(...: ): callable(...: ))) A coroutine function which executed the wrapped function with batches of input arguments. The wrapped function is called with concatenated arguments of multiple calls. Notes The decorator is not thread-safe, but only allows for concurrent access by async functions. The wrapped function may use multithreading, but only one thread at a time may call the function in order to avoid race conditions. The return value of the wrapped function must be a single iterable. All calls to the underlying function need to have the same number of positional arguments and the same keyword arguments. It also isn't possible to mix the two ways to pass an argument. The same argument always has to be passed either as keyword argument or as positional argument. Example >>> import asyncio >>> import dike ... ... >>> @dike.batch(target_batch_size=3, max_waiting_time=10) ... async def f(arg1, arg2): ... print(f\"arg1: {arg1}\") ... print(f\"arg2: {arg2}\") ... return [10, 11, 12] ... ... >>> async def main(): ... result = await asyncio.gather( ... f([0], [\"a\"]), ... f([1], [\"b\"]), ... f([2], [\"c\"]), ... ) ... ... print(f\"Result: {result}\") ... ... >>> asyncio.run(main()) arg1: [0, 1, 2] arg2: ['a', 'b', 'c'] Result: [[10], [11], [12]] function limit_jobs ( limit ) Decorator to limit the number of concurrent calls to a coroutine function. Not thread-safe. Parameters limit (int) \u2014 The maximum number of ongoing calls allowed at any time Returns (callable(...: )) The given coroutine function with added concurrency protection Raises TooManyCalls \u2014 The decorated function raises a dike.ToomanyCalls exception if it is called while already running limit times concurrently. ValueError \u2014 If the decorator is applied to something else than an async def function Note The decorator is not thread-safe, but only allows for concurrent access by async functions. The wrapped function may use multithreading, but only one thread at a time may call the function in order to avoid race conditions. Examples >>> import dike >>> import asyncio >>> import httpx >>> import dike ... ... >>> @dike.limit_jobs(limit=2) ... async def web_request(): ... async with httpx.AsyncClient() as client: ... response = await client.get(\"https://httpbin.org/status/200?sleep=100\") ... return response ... ... >>> async def main(): ... responses = await asyncio.gather( ... web_request(), web_request(), web_request(), return_exceptions=True ... ) ... for r in responses: ... if isinstance(r, dike._limit_jobs.TooManyCalls): ... print(\"too many calls\") ... else: ... print(r) ... ... >>> asyncio.run(main()) <Response [200 OK]> <Response [200 OK]> too many calls function retry ( attempts=None , exception_types=<class 'Exception'> , delay=None , backoff=1 , log_exception_info=True ) Decorator to limit the number of concurrent calls to a coroutine function. Not thread-safe. Parameters attempts (int, optional) \u2014 The maximum number of tries before re-raising the last exception. Per default there is no limit. exception_types (Union(type of baseexception, (type of baseexception)), optional) \u2014 The exception types which allow a retry. In case of other exceptions there is no retry. delay (timedelta, optional) \u2014 Delay between attempts. Per default there is no delay. backoff (int, optional) \u2014 Multiplier applied to the delay between attempts. Per default this is 1 , so that the delay is constant and does not grow. E.g. use 2 to double the delay with each attempt. log_exception_info (bool, optional) \u2014 Wether to include the exception stacktrace when logging any failed attempts. Default: True Returns (callable(callable(...: ): callable(...: ))) The given coroutine function with added exception handling and retry logic. Raises ValueError \u2014 For invalid configuration arguments. Examples >>> import asyncio >>> import httpx >>> import datetime >>> import dike ... >>> @dike.retry(attempts=2, delay=datetime.timedelta(milliseconds=10)) ... async def web_request(): ... async with httpx.AsyncClient() as client: ... response = await client.get(\"https://httpbin.org/status/400\") ... if response.status_code != httpx.codes.OK: ... raise RuntimeError(\"Request failed!\") ... return response ... ... >>> async def main(): ... response = await web_request() ... print(response) ... ... >>> asyncio.run(main()) ... # Log messages from two attempts: ... # WARNING:dike:Caught exception RuntimeError('Request failed!'). Retrying in 0.01s ... ... # Then: Traceback (most recent call last): ... RuntimeError: Request failed!","title":"Decorators"},{"location":"api/#exceptions","text":"class TooManyCalls ( ) Bases Exception BaseException Error raised by @limit_jobs when a call exceeds the preset limit.","title":"Exceptions"},{"location":"contributing/","text":"Contributing Contributions are welcome, and they are greatly appreciated! You can contribute in many ways: Types of Contributions Report Bugs Report bugs as Github issues . If you are reporting a bug, please include: Your operating system name and version. Any details about your local setup that might be helpful in troubleshooting. Detailed steps to reproduce the bug, ideally a minimal script which triggers the issue. Fix Bugs Look through the GitHub issues for bugs. Anything tagged with \"bug\" and \"help wanted\" is open to whoever wants to implement it. Implement Features Look through the GitHub issues for features. Anything tagged with \"enhancement\" and \"help wanted\" is open to whoever wants to implement it. Write Documentation dike could always use more documentation, whether as part of the official dike docs, in docstrings, or even on the web in blog posts, articles, and such. Submit Feedback The best way to send feedback is to file an issue on Github . If you are proposing a feature: Explain in detail how it would work. Keep the scope as narrow as possible, to make it easier to implement. Remember that this is a volunteer-driven project, and that contributions are welcome :) Get Started! Ready to contribute? Here's how to set up dike for local development. Fork the dike repo on GitHub. Clone your fork locally $ git clone git@github.com:your_name_here/dike.git Ensure poetry is installed. Install dependencies and start your virtualenv: $ poetry install --with=test,doc,dev Create a branch for local development: $ git checkout -b name-of-your-bugfix-or-feature Now you can make your changes locally. When you're done making changes, check that your changes pass the tests, including testing other Python versions, with tox: $ tox Commit your changes and push your branch to GitHub: $ git add . $ git commit -m \"Your detailed description of your changes.\" $ git push origin name-of-your-bugfix-or-feature Submit a pull request through the GitHub website. Pull Request Guidelines Before you submit a pull request, check that it meets these guidelines: The pull request should include tests. If the pull request adds functionality, the docs should be updated. Put your new functionality into a function with a docstring, and add the feature to the list in README.md. The pull request should work for CPython 3.7, 3.8 and 3.9. Check https://github.com/chr1st1ank/dike/actions and make sure that the tests pass for all supported Python versions. Deploying A reminder for the maintainers on how to deploy. On branch \"main\": - Adjust CHANGELOG.md as described on https://keepachangelog.com - Then run inv version [major | minor | patch] . This updates the version numbers and creates a tagged commit. - Push the commit to github: git push origin main && git push --tags . - A github action will automatically create a github release and publish to pypi.","title":"Contributing"},{"location":"contributing/#contributing","text":"Contributions are welcome, and they are greatly appreciated! You can contribute in many ways:","title":"Contributing"},{"location":"contributing/#types-of-contributions","text":"","title":"Types of Contributions"},{"location":"contributing/#report-bugs","text":"Report bugs as Github issues . If you are reporting a bug, please include: Your operating system name and version. Any details about your local setup that might be helpful in troubleshooting. Detailed steps to reproduce the bug, ideally a minimal script which triggers the issue.","title":"Report Bugs"},{"location":"contributing/#fix-bugs","text":"Look through the GitHub issues for bugs. Anything tagged with \"bug\" and \"help wanted\" is open to whoever wants to implement it.","title":"Fix Bugs"},{"location":"contributing/#implement-features","text":"Look through the GitHub issues for features. Anything tagged with \"enhancement\" and \"help wanted\" is open to whoever wants to implement it.","title":"Implement Features"},{"location":"contributing/#write-documentation","text":"dike could always use more documentation, whether as part of the official dike docs, in docstrings, or even on the web in blog posts, articles, and such.","title":"Write Documentation"},{"location":"contributing/#submit-feedback","text":"The best way to send feedback is to file an issue on Github . If you are proposing a feature: Explain in detail how it would work. Keep the scope as narrow as possible, to make it easier to implement. Remember that this is a volunteer-driven project, and that contributions are welcome :)","title":"Submit Feedback"},{"location":"contributing/#get-started","text":"Ready to contribute? Here's how to set up dike for local development. Fork the dike repo on GitHub. Clone your fork locally $ git clone git@github.com:your_name_here/dike.git Ensure poetry is installed. Install dependencies and start your virtualenv: $ poetry install --with=test,doc,dev Create a branch for local development: $ git checkout -b name-of-your-bugfix-or-feature Now you can make your changes locally. When you're done making changes, check that your changes pass the tests, including testing other Python versions, with tox: $ tox Commit your changes and push your branch to GitHub: $ git add . $ git commit -m \"Your detailed description of your changes.\" $ git push origin name-of-your-bugfix-or-feature Submit a pull request through the GitHub website.","title":"Get Started!"},{"location":"contributing/#pull-request-guidelines","text":"Before you submit a pull request, check that it meets these guidelines: The pull request should include tests. If the pull request adds functionality, the docs should be updated. Put your new functionality into a function with a docstring, and add the feature to the list in README.md. The pull request should work for CPython 3.7, 3.8 and 3.9. Check https://github.com/chr1st1ank/dike/actions and make sure that the tests pass for all supported Python versions.","title":"Pull Request Guidelines"},{"location":"contributing/#deploying","text":"A reminder for the maintainers on how to deploy. On branch \"main\": - Adjust CHANGELOG.md as described on https://keepachangelog.com - Then run inv version [major | minor | patch] . This updates the version numbers and creates a tagged commit. - Push the commit to github: git push origin main && git push --tags . - A github action will automatically create a github release and publish to pypi.","title":"Deploying"},{"location":"history/","text":"History Changelog in the style of keepachangelog.com . Types of changes: * 'Added' for new features. * 'Changed' for changes in existing functionality. * 'Deprecated' for soon-to-be removed features. * 'Removed' for now removed features. * 'Fixed' for any bug fixes. * 'Security' in case of vulnerabilities. The project uses semantic versioning. [Unreleased] [1.0.1] - 2024-10-15 Changed: Updates of dependencies and change of build tool to uv [1.0.0] - 2022-05-09 Added: Package API declared stable. [0.4.0] - 2022-05-08 Added: New @retry decorator for retries of coroutine calls. [0.3.1] - 2021-09-25 Added: Doctests to ensure the examples in the documentation keep being correct Fixed: A potential memory leak in case was fixed. It could occur if one of the tasks waiting for the result of a function decorated with @batch was cancelled. Changed: Decorators are now implemented in submodules. The public interface is unchanged [0.3.0] - 2021-07-15 Added: Support for numpy arrays in the @batch decorator [0.2.0] - 2021-07-15 Added: Propagation of function exceptions to all callers for the @batch decorator A dockerized example service using minibatching and load shedding Fixed: Occasional Python 3.8 f-strings were replaced by a Python 3.7 compatible implementation A race condition was fixed that occurred in case of multiple concurrent calls running into the max_waiting_time at the same moment [0.1.0] - 2021-07-13 Added: New @batch decorator for minibatching of coroutine calls. New @limit_jobs decorator to limit the number of concurrent calls to a coroutine function.","title":"History"},{"location":"history/#history","text":"Changelog in the style of keepachangelog.com . Types of changes: * 'Added' for new features. * 'Changed' for changes in existing functionality. * 'Deprecated' for soon-to-be removed features. * 'Removed' for now removed features. * 'Fixed' for any bug fixes. * 'Security' in case of vulnerabilities. The project uses semantic versioning.","title":"History"},{"location":"history/#unreleased","text":"","title":"[Unreleased]"},{"location":"history/#101-2024-10-15","text":"","title":"[1.0.1] - 2024-10-15"},{"location":"history/#changed","text":"Updates of dependencies and change of build tool to uv","title":"Changed:"},{"location":"history/#100-2022-05-09","text":"","title":"[1.0.0] - 2022-05-09"},{"location":"history/#added","text":"Package API declared stable.","title":"Added:"},{"location":"history/#040-2022-05-08","text":"","title":"[0.4.0] - 2022-05-08"},{"location":"history/#added_1","text":"New @retry decorator for retries of coroutine calls.","title":"Added:"},{"location":"history/#031-2021-09-25","text":"","title":"[0.3.1] - 2021-09-25"},{"location":"history/#added_2","text":"Doctests to ensure the examples in the documentation keep being correct","title":"Added:"},{"location":"history/#fixed","text":"A potential memory leak in case was fixed. It could occur if one of the tasks waiting for the result of a function decorated with @batch was cancelled.","title":"Fixed:"},{"location":"history/#changed_1","text":"Decorators are now implemented in submodules. The public interface is unchanged","title":"Changed:"},{"location":"history/#030-2021-07-15","text":"","title":"[0.3.0] - 2021-07-15"},{"location":"history/#added_3","text":"Support for numpy arrays in the @batch decorator","title":"Added:"},{"location":"history/#020-2021-07-15","text":"","title":"[0.2.0] - 2021-07-15"},{"location":"history/#added_4","text":"Propagation of function exceptions to all callers for the @batch decorator A dockerized example service using minibatching and load shedding","title":"Added:"},{"location":"history/#fixed_1","text":"Occasional Python 3.8 f-strings were replaced by a Python 3.7 compatible implementation A race condition was fixed that occurred in case of multiple concurrent calls running into the max_waiting_time at the same moment","title":"Fixed:"},{"location":"history/#010-2021-07-13","text":"","title":"[0.1.0] - 2021-07-13"},{"location":"history/#added_5","text":"New @batch decorator for minibatching of coroutine calls. New @limit_jobs decorator to limit the number of concurrent calls to a coroutine function.","title":"Added:"},{"location":"installation/","text":"Installation Stable release To install dike, run this command in your terminal: $ pip install dike This is the preferred method to install dike, as it will always install the most recent stable release. If you don't have pip installed, this Python installation guide can guide you through the process. From source The source for dike can be downloaded from the Github repo . You can either clone the public repository: $ git clone git://github.com/chr1st1ank/dike Or download the tarball : $ curl -OJL https://github.com/chr1st1ank/dike/tarball/main Once you have a copy of the source, you can install it with: $ pip install .","title":"Installation"},{"location":"installation/#installation","text":"","title":"Installation"},{"location":"installation/#stable-release","text":"To install dike, run this command in your terminal: $ pip install dike This is the preferred method to install dike, as it will always install the most recent stable release. If you don't have pip installed, this Python installation guide can guide you through the process.","title":"Stable release"},{"location":"installation/#from-source","text":"The source for dike can be downloaded from the Github repo . You can either clone the public repository: $ git clone git://github.com/chr1st1ank/dike Or download the tarball : $ curl -OJL https://github.com/chr1st1ank/dike/tarball/main Once you have a copy of the source, you can install it with: $ pip install .","title":"From source"},{"location":"usage/","text":"Usage To use dike in a project import dike Managing CPU bound work in a process pool import asyncio import concurrent import dike from my_project import cpu_bound_function pool = concurrent.futures.ProcessPoolExecutor(max_workers=2) @dike.limit_jobs(limit=4) def calculate_in_pool(args): \"\"\"Run calculations of cpu_bound_function() in a process pool with limited queue\"\"\" loop = asyncio.get_event_loop() return await loop.run_in_executor(_pool, cpu_bound_function, *args) Minibatching the inputs of a machine learning model","title":"Usage"},{"location":"usage/#usage","text":"To use dike in a project import dike","title":"Usage"},{"location":"usage/#managing-cpu-bound-work-in-a-process-pool","text":"import asyncio import concurrent import dike from my_project import cpu_bound_function pool = concurrent.futures.ProcessPoolExecutor(max_workers=2) @dike.limit_jobs(limit=4) def calculate_in_pool(args): \"\"\"Run calculations of cpu_bound_function() in a process pool with limited queue\"\"\" loop = asyncio.get_event_loop() return await loop.run_in_executor(_pool, cpu_bound_function, *args)","title":"Managing CPU bound work in a process pool"},{"location":"usage/#minibatching-the-inputs-of-a-machine-learning-model","text":"","title":"Minibatching the inputs of a machine learning model"}]}