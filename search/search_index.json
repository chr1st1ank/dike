{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"dike Python asyncio tools for web service resilience Documentation: https://chr1st1ank.github.io/dike/ License: Apache-2.0 Features Concurrency limiting for asynchronous functions The @limit_jobs decorator allows to limit the number of concurrent excecutions of a coroutine function. This can be useful for limiting queueing times or for limiting the load put onto backend services. Example with an external web request using the httpx library: import asyncio import httpx import dike @dike . limit_jobs ( limit = 2 ) async def web_request (): async with httpx . AsyncClient () as client : response = await client . get ( \"https://httpstat.us/200?sleep=100\" ) return response async def main (): responses = await asyncio . gather ( web_request (), web_request (), web_request (), return_exceptions = True ) for r in responses : if isinstance ( r , dike . TooManyCalls ): print ( \"too many calls\" ) else : print ( r ) asyncio . run ( main ()) The output shows that the first two requests succeed. The third one hits the concurrency limit: <Response [200 OK]> <Response [200 OK]> too many calls Mini-batching for asynchronous function calls The @batch decorator groups function calls into batches and only calls the wrapped function with the aggregated input. This is useful if the function scales well with the size of the input arguments but you're getting the input data in smaller bits, e.g. as individual HTTP requests. Example: import asyncio import dike @dike . batch ( target_batch_size = 3 , max_waiting_time = 10 ) async def f ( arg1 , arg2 ): print ( f \"arg1: { arg1 } \" ) print ( f \"arg2: { arg2 } \" ) return [ 10 , 11 , 12 ] async def main (): result = await asyncio . gather ( f ([ 0 ], [ \"a\" ]), f ([ 1 ], [ \"b\" ]), f ([ 2 ], [ \"c\" ]), ) print ( f \"Result: { result } \" ) asyncio . run ( main ()) Output: arg1: [0, 1, 2] arg2: ['a', 'b', 'c'] Result: [[10], [11], [12]] Installation Simply install from pypi. The library is pure Python without any dependencies other than the standard library. pip install dike","title":"Home"},{"location":"#dike","text":"Python asyncio tools for web service resilience Documentation: https://chr1st1ank.github.io/dike/ License: Apache-2.0","title":"dike"},{"location":"#features","text":"","title":"Features"},{"location":"#concurrency-limiting-for-asynchronous-functions","text":"The @limit_jobs decorator allows to limit the number of concurrent excecutions of a coroutine function. This can be useful for limiting queueing times or for limiting the load put onto backend services. Example with an external web request using the httpx library: import asyncio import httpx import dike @dike . limit_jobs ( limit = 2 ) async def web_request (): async with httpx . AsyncClient () as client : response = await client . get ( \"https://httpstat.us/200?sleep=100\" ) return response async def main (): responses = await asyncio . gather ( web_request (), web_request (), web_request (), return_exceptions = True ) for r in responses : if isinstance ( r , dike . TooManyCalls ): print ( \"too many calls\" ) else : print ( r ) asyncio . run ( main ()) The output shows that the first two requests succeed. The third one hits the concurrency limit: <Response [200 OK]> <Response [200 OK]> too many calls","title":"Concurrency limiting for asynchronous functions"},{"location":"#mini-batching-for-asynchronous-function-calls","text":"The @batch decorator groups function calls into batches and only calls the wrapped function with the aggregated input. This is useful if the function scales well with the size of the input arguments but you're getting the input data in smaller bits, e.g. as individual HTTP requests. Example: import asyncio import dike @dike . batch ( target_batch_size = 3 , max_waiting_time = 10 ) async def f ( arg1 , arg2 ): print ( f \"arg1: { arg1 } \" ) print ( f \"arg2: { arg2 } \" ) return [ 10 , 11 , 12 ] async def main (): result = await asyncio . gather ( f ([ 0 ], [ \"a\" ]), f ([ 1 ], [ \"b\" ]), f ([ 2 ], [ \"c\" ]), ) print ( f \"Result: { result } \" ) asyncio . run ( main ()) Output: arg1: [0, 1, 2] arg2: ['a', 'b', 'c'] Result: [[10], [11], [12]]","title":"Mini-batching for asynchronous function calls"},{"location":"#installation","text":"Simply install from pypi. The library is pure Python without any dependencies other than the standard library. pip install dike","title":"Installation"},{"location":"api/","text":"API documentation Decorators function limit_jobs ( limit ) Decorator to limit the number of concurrent calls to a coroutine function. Parameters limit (int) \u2014 The maximum number of ongoing calls allowed at any time Returns The given coroutine function with added concurrency protection Raises TooManyCalls \u2014 The decorated function raises a dike.ToomanyCalls exception if it is called while already running limit times concurrently. ValueError \u2014 If the decorator is applied to something else than an async def function Examples >>> import asyncio >>> import httpx >>> import dike ... ... >>> @dike . limit_jobs ( limit = 2 ) ... async def web_request (): ... async with httpx . AsyncClient () as client : ... response = await client . get ( \"https://httpstat.us/200?sleep=100\" ) ... return response ... ... >>> async def main (): ... responses = await asyncio . gather ( ... web_request (), web_request (), web_request (), return_exceptions = True ... ) ... for r in responses : ... if isinstance ( r , dike . TooManyCalls ): ... print ( \"too many calls\" ) ... else : ... print ( r ) ... ... >>> asyncio . run ( main ()) < Response [ 200 OK ] > < Response [ 200 OK ] > too many calls function batch ( target_batch_size , max_waiting_time , max_processing_time=10.0 ) @batch is a decorator to cumulate function calls and process them in batches. Parameters target_batch_size (int) \u2014 As soon as the collected function arguments reach target_batch_size, the wrapped function is called and the results are returned. Note that the function may also be called with longer arguments than target_batch_size. max_waiting_time (float) \u2014 Maximum waiting time before calling the underlying function although the target_batch_size hasn't been reached. max_processing_time (float, optional) \u2014 Maximum time for the processing itself (without waiting) before an asyncio.TimeoutError is raised. Note: It is strongly advised to set a reasonably strict timeout here in order not to create starving tasks which never finish in case something is wrong with the backend call. Raises ValueError \u2014 If the arguments target_batch_size or max_waiting time are not >= 0. asyncio.TimeoutError \u2014 Is raised when calling the wrapped function takes longer than max_processing_time Returns A coroutine function which executed the wrapped function with batches of input arguments. The wrapped function is called with concatenated arguments of multiple calls. Note The return value of the wrapped function must be a single iterable All calls to the underlying function need to have the same number of positional arguments and keyword arguments Example >>> import asyncio >>> import dike ... ... >>> @dike . batch ( target_batch_size = 3 , max_waiting_time = 10 ) ... async def f ( arg1 , arg2 ): ... print ( f \"arg1: { arg1 } \" ) ... print ( f \"arg2: { arg2 } \" ) ... return [ 10 , 11 , 12 ] ... ... >>> async def main (): ... result = await asyncio . gather ( ... f ([ 0 ], [ \"a\" ]), ... f ([ 1 ], [ \"b\" ]), ... f ([ 2 ], [ \"c\" ]), ... ) ... ... print ( f \"Result: { result } \" ) ... ... >>> asyncio . run ( main ()) arg1 : [ 0 , 1 , 2 ] arg2 : [ 'a' , 'b' , 'c' ] Result : [[ 10 ], [ 11 ], [ 12 ]] Exceptions class TooManyCalls ( ) Bases Exception BaseException Error raised by @limit_jobs when a call exceeds the preset limit","title":"API documentation"},{"location":"api/#api-documentation","text":"","title":"API documentation"},{"location":"api/#decorators","text":"function limit_jobs ( limit ) Decorator to limit the number of concurrent calls to a coroutine function. Parameters limit (int) \u2014 The maximum number of ongoing calls allowed at any time Returns The given coroutine function with added concurrency protection Raises TooManyCalls \u2014 The decorated function raises a dike.ToomanyCalls exception if it is called while already running limit times concurrently. ValueError \u2014 If the decorator is applied to something else than an async def function Examples >>> import asyncio >>> import httpx >>> import dike ... ... >>> @dike . limit_jobs ( limit = 2 ) ... async def web_request (): ... async with httpx . AsyncClient () as client : ... response = await client . get ( \"https://httpstat.us/200?sleep=100\" ) ... return response ... ... >>> async def main (): ... responses = await asyncio . gather ( ... web_request (), web_request (), web_request (), return_exceptions = True ... ) ... for r in responses : ... if isinstance ( r , dike . TooManyCalls ): ... print ( \"too many calls\" ) ... else : ... print ( r ) ... ... >>> asyncio . run ( main ()) < Response [ 200 OK ] > < Response [ 200 OK ] > too many calls function batch ( target_batch_size , max_waiting_time , max_processing_time=10.0 ) @batch is a decorator to cumulate function calls and process them in batches. Parameters target_batch_size (int) \u2014 As soon as the collected function arguments reach target_batch_size, the wrapped function is called and the results are returned. Note that the function may also be called with longer arguments than target_batch_size. max_waiting_time (float) \u2014 Maximum waiting time before calling the underlying function although the target_batch_size hasn't been reached. max_processing_time (float, optional) \u2014 Maximum time for the processing itself (without waiting) before an asyncio.TimeoutError is raised. Note: It is strongly advised to set a reasonably strict timeout here in order not to create starving tasks which never finish in case something is wrong with the backend call. Raises ValueError \u2014 If the arguments target_batch_size or max_waiting time are not >= 0. asyncio.TimeoutError \u2014 Is raised when calling the wrapped function takes longer than max_processing_time Returns A coroutine function which executed the wrapped function with batches of input arguments. The wrapped function is called with concatenated arguments of multiple calls. Note The return value of the wrapped function must be a single iterable All calls to the underlying function need to have the same number of positional arguments and keyword arguments Example >>> import asyncio >>> import dike ... ... >>> @dike . batch ( target_batch_size = 3 , max_waiting_time = 10 ) ... async def f ( arg1 , arg2 ): ... print ( f \"arg1: { arg1 } \" ) ... print ( f \"arg2: { arg2 } \" ) ... return [ 10 , 11 , 12 ] ... ... >>> async def main (): ... result = await asyncio . gather ( ... f ([ 0 ], [ \"a\" ]), ... f ([ 1 ], [ \"b\" ]), ... f ([ 2 ], [ \"c\" ]), ... ) ... ... print ( f \"Result: { result } \" ) ... ... >>> asyncio . run ( main ()) arg1 : [ 0 , 1 , 2 ] arg2 : [ 'a' , 'b' , 'c' ] Result : [[ 10 ], [ 11 ], [ 12 ]]","title":"Decorators"},{"location":"api/#exceptions","text":"class TooManyCalls ( ) Bases Exception BaseException Error raised by @limit_jobs when a call exceeds the preset limit","title":"Exceptions"},{"location":"contributing/","text":"Contributing Contributions are welcome, and they are greatly appreciated! You can contribute in many ways: Types of Contributions Report Bugs Report bugs at https://github.com/chr1st1ank/dike/issues. If you are reporting a bug, please include: Your operating system name and version. Any details about your local setup that might be helpful in troubleshooting. Detailed steps to reproduce the bug, ideally a minimal script which triggers the issue. Fix Bugs Look through the GitHub issues for bugs. Anything tagged with \"bug\" and \"help wanted\" is open to whoever wants to implement it. Implement Features Look through the GitHub issues for features. Anything tagged with \"enhancement\" and \"help wanted\" is open to whoever wants to implement it. Write Documentation dike could always use more documentation, whether as part of the official dike docs, in docstrings, or even on the web in blog posts, articles, and such. Submit Feedback The best way to send feedback is to file an issue at https://github.com/chr1st1ank/dike/issues. If you are proposing a feature: Explain in detail how it would work. Keep the scope as narrow as possible, to make it easier to implement. Remember that this is a volunteer-driven project, and that contributions are welcome :) Get Started! Ready to contribute? Here's how to set up dike for local development. Fork the dike repo on GitHub. Clone your fork locally $ git clone git@github.com:your_name_here/dike.git Ensure poetry is installed. Install dependencies and start your virtualenv: $ poetry install -E test -E doc -E dev Create a branch for local development: $ git checkout -b name-of-your-bugfix-or-feature Now you can make your changes locally. When you're done making changes, check that your changes pass the tests, including testing other Python versions, with tox: $ tox Commit your changes and push your branch to GitHub: $ git add . $ git commit -m \"Your detailed description of your changes.\" $ git push origin name-of-your-bugfix-or-feature Submit a pull request through the GitHub website. Pull Request Guidelines Before you submit a pull request, check that it meets these guidelines: The pull request should include tests. If the pull request adds functionality, the docs should be updated. Put your new functionality into a function with a docstring, and add the feature to the list in README.md. The pull request should work for CPython 3.7, 3.8 and 3.9. Check https://github.com/chr1st1ank/dike/actions and make sure that the tests pass for all supported Python versions. Deploying A reminder for the maintainers on how to deploy. On branch \"main\": - Adjust CHANGELOG.md as described on https://keepachangelog.com - Adjust the version number in dike/ init .py - Then run poetry version [major | minor | patch] - Commit and push the changes - Create a github release and watch the release workflow publishing the documentation and the PyPI package. Credits Creator Christian Krudewig chr1st1ank@krudewig-online.de Contributors None yet. Why not be the first?","title":"Contributing"},{"location":"contributing/#contributing","text":"Contributions are welcome, and they are greatly appreciated! You can contribute in many ways:","title":"Contributing"},{"location":"contributing/#types-of-contributions","text":"","title":"Types of Contributions"},{"location":"contributing/#report-bugs","text":"Report bugs at https://github.com/chr1st1ank/dike/issues. If you are reporting a bug, please include: Your operating system name and version. Any details about your local setup that might be helpful in troubleshooting. Detailed steps to reproduce the bug, ideally a minimal script which triggers the issue.","title":"Report Bugs"},{"location":"contributing/#fix-bugs","text":"Look through the GitHub issues for bugs. Anything tagged with \"bug\" and \"help wanted\" is open to whoever wants to implement it.","title":"Fix Bugs"},{"location":"contributing/#implement-features","text":"Look through the GitHub issues for features. Anything tagged with \"enhancement\" and \"help wanted\" is open to whoever wants to implement it.","title":"Implement Features"},{"location":"contributing/#write-documentation","text":"dike could always use more documentation, whether as part of the official dike docs, in docstrings, or even on the web in blog posts, articles, and such.","title":"Write Documentation"},{"location":"contributing/#submit-feedback","text":"The best way to send feedback is to file an issue at https://github.com/chr1st1ank/dike/issues. If you are proposing a feature: Explain in detail how it would work. Keep the scope as narrow as possible, to make it easier to implement. Remember that this is a volunteer-driven project, and that contributions are welcome :)","title":"Submit Feedback"},{"location":"contributing/#get-started","text":"Ready to contribute? Here's how to set up dike for local development. Fork the dike repo on GitHub. Clone your fork locally $ git clone git@github.com:your_name_here/dike.git Ensure poetry is installed. Install dependencies and start your virtualenv: $ poetry install -E test -E doc -E dev Create a branch for local development: $ git checkout -b name-of-your-bugfix-or-feature Now you can make your changes locally. When you're done making changes, check that your changes pass the tests, including testing other Python versions, with tox: $ tox Commit your changes and push your branch to GitHub: $ git add . $ git commit -m \"Your detailed description of your changes.\" $ git push origin name-of-your-bugfix-or-feature Submit a pull request through the GitHub website.","title":"Get Started!"},{"location":"contributing/#pull-request-guidelines","text":"Before you submit a pull request, check that it meets these guidelines: The pull request should include tests. If the pull request adds functionality, the docs should be updated. Put your new functionality into a function with a docstring, and add the feature to the list in README.md. The pull request should work for CPython 3.7, 3.8 and 3.9. Check https://github.com/chr1st1ank/dike/actions and make sure that the tests pass for all supported Python versions.","title":"Pull Request Guidelines"},{"location":"contributing/#deploying","text":"A reminder for the maintainers on how to deploy. On branch \"main\": - Adjust CHANGELOG.md as described on https://keepachangelog.com - Adjust the version number in dike/ init .py - Then run poetry version [major | minor | patch] - Commit and push the changes - Create a github release and watch the release workflow publishing the documentation and the PyPI package.","title":"Deploying"},{"location":"contributing/#credits","text":"","title":"Credits"},{"location":"contributing/#creator","text":"Christian Krudewig chr1st1ank@krudewig-online.de","title":"Creator"},{"location":"contributing/#contributors","text":"None yet. Why not be the first?","title":"Contributors"},{"location":"history/","text":"History Unreleased 0.2.0 (2021-07-15) Added: * Propagation of function exceptions to all callers for the @batch decorator * A dockerized example service using minibatching and load shedding Fixed: * Occasional Python 3.8 f-strings were replaced by a Python 3.7 compatible implementation * A race condition was fixed that occurred in case of multiple concurrent calls running into the max_waiting_time at the same moment 0.1.0 (2021-07-13) Added: * @batch decorator for minibatching of coroutine calls. * @limit_jobs decorator to limit the number of concurrent calls to a coroutine function.","title":"History"},{"location":"history/#history","text":"","title":"History"},{"location":"history/#unreleased","text":"","title":"Unreleased"},{"location":"history/#020-2021-07-15","text":"Added: * Propagation of function exceptions to all callers for the @batch decorator * A dockerized example service using minibatching and load shedding Fixed: * Occasional Python 3.8 f-strings were replaced by a Python 3.7 compatible implementation * A race condition was fixed that occurred in case of multiple concurrent calls running into the max_waiting_time at the same moment","title":"0.2.0 (2021-07-15)"},{"location":"history/#010-2021-07-13","text":"Added: * @batch decorator for minibatching of coroutine calls. * @limit_jobs decorator to limit the number of concurrent calls to a coroutine function.","title":"0.1.0 (2021-07-13)"},{"location":"installation/","text":"Installation Stable release To install dike, run this command in your terminal: $ pip install dike This is the preferred method to install dike, as it will always install the most recent stable release. If you don't have pip installed, this Python installation guide can guide you through the process. From source The source for dike can be downloaded from the Github repo . You can either clone the public repository: $ git clone git://github.com/chr1st1ank/dike Or download the tarball : $ curl -OJL https://github.com/chr1st1ank/dike/tarball/main Once you have a copy of the source, you can install it with: $ pip install .","title":"Installation"},{"location":"installation/#installation","text":"","title":"Installation"},{"location":"installation/#stable-release","text":"To install dike, run this command in your terminal: $ pip install dike This is the preferred method to install dike, as it will always install the most recent stable release. If you don't have pip installed, this Python installation guide can guide you through the process.","title":"Stable release"},{"location":"installation/#from-source","text":"The source for dike can be downloaded from the Github repo . You can either clone the public repository: $ git clone git://github.com/chr1st1ank/dike Or download the tarball : $ curl -OJL https://github.com/chr1st1ank/dike/tarball/main Once you have a copy of the source, you can install it with: $ pip install .","title":"From source"},{"location":"usage/","text":"Usage To use dike in a project import dike Managing CPU bound work in a process pool import asyncio import concurrent import dike from my_project import cpu_bound_function pool = concurrent . futures . ProcessPoolExecutor ( max_workers = 2 ) @dike . limit_jobs ( limit = 4 ) def calculate_in_pool ( args ): \"\"\"Run calculations of cpu_bound_function() in a process pool with limited queue\"\"\" loop = asyncio . get_event_loop () return await loop . run_in_executor ( _pool , cpu_bound_function , * args ) Minibatching the inputs of a machine learning model","title":"Usage"},{"location":"usage/#usage","text":"To use dike in a project import dike","title":"Usage"},{"location":"usage/#managing-cpu-bound-work-in-a-process-pool","text":"import asyncio import concurrent import dike from my_project import cpu_bound_function pool = concurrent . futures . ProcessPoolExecutor ( max_workers = 2 ) @dike . limit_jobs ( limit = 4 ) def calculate_in_pool ( args ): \"\"\"Run calculations of cpu_bound_function() in a process pool with limited queue\"\"\" loop = asyncio . get_event_loop () return await loop . run_in_executor ( _pool , cpu_bound_function , * args )","title":"Managing CPU bound work in a process pool"},{"location":"usage/#minibatching-the-inputs-of-a-machine-learning-model","text":"","title":"Minibatching the inputs of a machine learning model"}]}